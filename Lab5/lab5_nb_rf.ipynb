{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Naive Bayes and Random Forests\n",
    "In this lab we'll learn to use [GridSearchCV] and take a closer look the [Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) and [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes widely known for being effective for spam email classification. We'll try our hand at applying NB on the [Spam email dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI data repository. Download the dataset and move it to your lab data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/SMSSpamCollection'\n",
    "df = pd.read_table(fname, names=['class', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "Given collection of emails $d^{(1)}, d^{(2)}, ..., d^{(N)}$ with corresponding labels $y^{(i)}$, we first need to extract features from each document. We'll do this by computing word counts of each word in each document. This is commonly referred to as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation.\n",
    "\n",
    "After doing some preprocessing of our data, we'll have: $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, where:\n",
    "* $x^{(i)} \\in R^d$ is a word count feature vector for document $i$, and $d$ is the size of the vocabulary. For the purposes of this lab, we define our vocabulary to be the set of words that appear in our dataset of documents.\n",
    "$$ x^{(i)} = (\\theta^{(i)}_1, \\theta^{(i)}_2, ..., \\theta^{(i)}_d)$$\n",
    "$$\\theta^{(i)}_k = \\text{ the number of times word $k$ appears in document $i$}$$\n",
    "\n",
    "\n",
    "* $y^{(i)} \\in \\{0, 1\\}$ is its class, where $0$ denotes spam, and $1$ denotes real email.\n",
    "\n",
    "First we need to compute word count feature vectors. We can do this manually like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def get_all_words(docs):\n",
    "    '''\n",
    "    docs: pandas.Series of strings\n",
    "    Returns: set of words(strings)\n",
    "    '''\n",
    "    # Compute all the words in the docs\n",
    "    words = set()\n",
    "    for d in docs:\n",
    "        # this will strip punctuation\n",
    "        # ref: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        d = d.translate(string.punctuation)\n",
    "        words = words.union(set(d.split()))\n",
    "    return words\n",
    "\n",
    "def make_word_cnt_feats(docs, vocabulary=None):\n",
    "    '''\n",
    "    docs: iterable(ie: list or pandas.Series) of strings\n",
    "    vocabulary: set of words(strings) that appear in docs\n",
    "    Returns: numpy matrix of the word count features\n",
    "    '''\n",
    "    if vocabulary is None:\n",
    "        vocabulary = get_all_words(docs)\n",
    "    words = sorted(vocabulary)\n",
    "    word_dict = {w: idx for idx, w in enumerate(words)}\n",
    "    \n",
    "    # feature matrix will be of size n x (size of vocabulary + 1)\n",
    "    # add 1 to account for words that dont appear in the vocabulary to avoid random issues\n",
    "    vocab_size = len(words)\n",
    "    word_cnt_features = np.zeros((len(docs), len(words) + 1))\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx]\n",
    "        doc = doc.translate(string.punctuation)\n",
    "        words = doc.split()\n",
    "        for w in words:\n",
    "            word_idx = word_dict.get(w, vocab_size)\n",
    "            word_cnt_features[idx, word_idx] += 1\n",
    "            \n",
    "    return word_cnt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5572, 15693)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_all_words(df['text'])\n",
    "feature_matrix = make_word_cnt_feats(df['text'], vocabulary)\n",
    "print('Feature matrix shape: {}'.format(feature_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that sklearn actually has a built-in method to do this kind of word count featurization! (But of course, it's good to get your hands dirty with manual featurization from time to time to understand all the ingredients of these algorithms). See sklearn's [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function for more details.                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "word_cnt_feats = count_vectorizer.fit_transform(df['text'])\n",
    "word_cnt_feats\n",
    "# looks like this gives us fewer columns, which means we missed\n",
    "# a bunch of unwanted terms(maybe words with numbers?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) has a bunch of useful functions that you should checkout. The important ones that we'll use are:\n",
    "* [fit](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit): this learns the vocabulary of the input collection of strings/\"documents\"\n",
    "* [transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform): given a collection of documents, this produces the word count features for the documents\n",
    "* fit_transform: calls fit, then transform\n",
    "\n",
    "Note that the word count feature matrix(aka document-term matrix) is in sparse format. This is not an issue for the sklearn models. But if you want to expand them to non-sparse numpy arrays, use the toarray function. Note that this can be problematic for bigger datasets! We can somewhat get around this b/c this dataset is small enough that we can fully expand a 5k by 8k matrix(since it's sparse enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = word_cnt_feats.toarray()\n",
    "Y = (df['class'] == 'ham').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d8e3baaa4d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "Y.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the document-term matrix format, we can use the Naive Bayes classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.978\n",
      "F1 Score: 0.987\n",
      "ROC AUC Score: 0.947\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}'.format(accuracy_score(preds, y_test)))\n",
    "print('F1 Score: {:.3f}'.format(f1_score(preds, y_test)))\n",
    "print('ROC AUC Score: {:.3f}'.format(roc_auc_score(preds, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Tasks\n",
    "Now it's your turn to use it on the Naive Bayes model. Pick an evaluation metric to use for cross validation like F1 score, ROC AUC score, etc. In the spam classification setting, do you think it makes sense to optimize for higher precision? recall? f1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Note that the code above fixed alpha to 1. But as we've learned in class and last week's lab, we should use cross validation for any kind of parameter search. Use [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to do the cross validation over 5 folds to find the alpha that works best on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.9896551724137931,\n",
       " (0, 5): 0.9788672865595943,\n",
       " (0, 10): 0.9731993299832497,\n",
       " (0, 25): 0.9555921052631579,\n",
       " (0, 100): 0.9318364073777066,\n",
       " (1, 1): 0.9913793103448276,\n",
       " (1, 5): 0.9690376569037658,\n",
       " (1, 10): 0.9586776859504132,\n",
       " (1, 25): 0.9392712550607287,\n",
       " (1, 100): 0.9309791332263242,\n",
       " (2, 1): 0.9914383561643836,\n",
       " (2, 5): 0.9806560134566863,\n",
       " (2, 10): 0.96843853820598,\n",
       " (2, 25): 0.9502852485737571,\n",
       " (2, 100): 0.9350441058540497,\n",
       " (3, 1): 0.9929701230228472,\n",
       " (3, 5): 0.9801894918173988,\n",
       " (3, 10): 0.9685106382978723,\n",
       " (3, 25): 0.9443983402489626,\n",
       " (3, 100): 0.9229521492295215,\n",
       " (4, 1): 0.9868536371603857,\n",
       " (4, 5): 0.9734816082121472,\n",
       " (4, 10): 0.9538977367979882,\n",
       " (4, 25): 0.9366255144032921,\n",
       " (4, 100): 0.9229521492295215}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pdb\n",
    "alphas = [1, 5, 10, 25, 100] # you should vary these \n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "def naive_bayes_kfold(X_train, y_train, alphas, kf):\n",
    "    cv_split_results = {} # store your cross validation results\n",
    "\n",
    "    for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_train)):\n",
    "        x_split_train, x_split_test = X_train[train_idx], X_train[test_idx]\n",
    "        y_split_train, y_split_test = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "        for a in alphas:\n",
    "            nb = MultinomialNB(alpha=a)\n",
    "            nb.fit(x_split_train, y_split_train)\n",
    "            preds = nb.predict(x_split_test)\n",
    "            f1 = f1_score(preds, y_split_test)\n",
    "            cv_split_results[fold_num,a] =  cv_split_results.get(a, 0) + f1\n",
    "            \n",
    "    return cv_split_results\n",
    "\n",
    "naive_bayes_kfold(X_train, y_train, alphas, kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Now that you've found the optimal alpha using cross validation over the training set, evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873678783191544"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha = 1 performs best\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)\n",
    "f1 = f1_score(preds, y_test)\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Questions:\n",
    "* Naive Bayes has a seemingly stringent independence assumption. Do you think the variables in this email classification problem(word count vectors) are really independent? Does this matter? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Two weeks ago, we played around with decision trees. In general, practictioners almost never use decision trees over Random Forests. Recall from lecture, that Random Forests takes some number of subsamples of the data:\n",
    "$S_1, S_2, ..., S_k \\subseteq \\{(x^{(i)}, y^{(i)}\\}_{i=0}^N$. On each subsample of the data, $S_i$, the Random Forests algorithm trains a decision tree on $S_i$.\n",
    "\n",
    "At classification time, when we get a datapoint $x$, the Random Forests classifier runs $x$ through decision trees $D_1, D_2, ..., D_k$, which produces a \"probability\" estimate of each class:\n",
    "\n",
    "$$P(y = 1 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "$$P(y = 0 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "Then from there, we get a class prediction by picking $y = 1$ if $P(y = 1 | x) > T$, for some threshold probability $T$.\n",
    "\n",
    "\n",
    "Instead of using the spam dataset to try out Random Forests, we'll re-use the credit loan data from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever data cleaning, data imputing, etc you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Parameters\n",
    "\n",
    "The Random Forest parameters are more or less the same as those for Decision Trees. The only additional parameter is n_estimators, which is the number of decision trees to grow for the algorithm.\n",
    "* n_estimators : integer, the number of trees in the forest.\n",
    "* criterion : string,  “gini” or “entropy” \n",
    "* max_features : int, float, string or None, the number of features to consider when looking for the best split\n",
    "* max_depth : integer, the maximum depth of the tree\n",
    "* min_samples_split : int, float, the minimum number of samples required to split an internal node:\n",
    "* min_samples_leaf : int, float, optional (default=1), the minimum number of samples required to be at a leaf node:\n",
    "* min_weight_fraction_leaf : float, optional \n",
    "* max_leaf_nodes : int or None, optional (default=None)\n",
    "\n",
    "0) What do you think will be the effect of increasing the n\\_estimators parameter? If n_estimators = 1, is this any different to using a decision tree classifier(assuming you set all the other parameters like min_sample_split, max_depth, etc the same way in both cases)?\n",
    "\n",
    "1) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Pick a few values for each of the random forest parameters for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param settings to try out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last weeks lab, we saw how to grid search over all possible model parameters and cross validation folds. To refresh your memory, below is a slightly updated version of that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of a manual grid search for LogisticRegression on random data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def sample_manual_grid_search(n_samples, n_feats, splits):\n",
    "    # make the fake data\n",
    "    X = np.random.randint(0, 10, (n_samples, n_feats))\n",
    "    Y = (np.random.random(n_samples) > 0.5) # random true/false\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "    \n",
    "    # params to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    rows = [] # store dicts of the cross validation results for each fold\n",
    "    for p in penalties:\n",
    "        for c in c_vals:\n",
    "\n",
    "            fold_dict = {\n",
    "                    # NOTE: these key values must be exactly as they're spelled in\n",
    "                    # the sklearn model!\n",
    "                    'penalty': p,\n",
    "                    'C': c,\n",
    "            }\n",
    "            fold_results = {}\n",
    "            for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "                x_split_train, x_split_test = X_train[train_idx], X_train[test_idx]\n",
    "                y_split_train, y_split_test = y_train[train_idx], y_train[test_idx]\n",
    "                    \n",
    "                model = LogisticRegression(**fold_dict)\n",
    "                model.fit(x_split_train, y_split_train)\n",
    "                pred_probs = model.predict_proba(x_split_test)\n",
    "                loss = roc_auc_score(y_split_test, pred_probs[:, 0])\n",
    "                fold_results['fold_{}'.format(fold_num+1)] = loss\n",
    "                    \n",
    "                # now that we have all the fold results, add them to the dataframe\n",
    "            fold_dict.update(fold_results)\n",
    "            fold_dict['avg_score'] = np.mean(fold_results.values())\n",
    "            rows.append(fold_dict)\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_results = sample_manual_grid_search(n_samples=100, n_feats=20, splits=5)\n",
    "print(\"Grid Search Results for sample data with Logistic Regression:\")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Do the same kind of grid search cross validation for Random Forests classifier. Which set of parameters gets the best average score over all folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your grid search CV for RandomForests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing 8x nested for-loops to search over all parameters can be a bit cumbersome. Luckily, sklearn has a very nice [GridSearchCV class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) that can do all this grid searching and cross validating for us!\n",
    "\n",
    "\n",
    "## Grid Search Cross Validation\n",
    "GridSearchCV requires:\n",
    "* an estimator object(IE: LogisticRegression(), SVC(), KNeighborsClassifier(), etc)\n",
    "* param_grid: a dictionary mapping parameter names to lists of values to search over. We'll see an example below\n",
    "* scoring: a string. See the valid inputs here: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "* n_jobs: number of jobs to run in parallel to speed up the computation. GridSearchCV can take a long time if you have a lot of different parameter combinations to try out. So it's important to set this parameter if your laptop/desktop has a lot of cores that you want to take advantage of.\n",
    "* cv: int, number of cross validation folds to use\n",
    "* refit: boolean, if true, at the end of the Grid Search, GridSearchCV will refit the entire data with a model using the optimal parameters(you can then access this with the GridSearchCV.best\\_parameters_ attribute).\n",
    "\n",
    "Here's an example of how we'd use it for some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of how to use GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def sample_cross_val_param_search():\n",
    "    # parameters to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "    param_grid = {'penalty': penalties, 'C': c_vals}\n",
    "    \n",
    "    # make some fake data\n",
    "    n_samples = 1000\n",
    "    feats = 10\n",
    "    x_train = np.random.random((n_samples, feats))\n",
    "    y_train = (np.random.random(n_samples) > 0.5) # randomly generate True/False labels\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, 'f1', cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return grid\n",
    "grid = sample_cross_val_param_search()\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(\"GridSearchCV results:\")\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best estimator: {}'.format(grid.best_estimator_))\n",
    "print('Best Parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Use GridSearchCV to do the same parameter search you did above. What is the best performing set of parameters? Do they match up with what you had above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) As mentioned earlier, GridSearchCV.best\\_estimator\\_ will contain a model with parameters set to the best performing set of parameters fitted on the entire training set. Use this best estimator on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Random Forests has a [feature importances](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) attribute. Which features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Rerun the classifier with a different scoring metric. Do the feature importances change? Which features are most important now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://etav.github.io/projects/spam_message_classifier_naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
